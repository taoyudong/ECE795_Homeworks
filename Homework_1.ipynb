{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_1",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# **Running Pyspark in Colab**\n",
        "PySpark is the Python API for Spark, which is an important tool in Big Data. To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 2.4.4 with hadoop 2.7, Java 8 and Findspark to locate the spark in the system. The tools installation can be carried out inside the Jupyter Notebook of the Colab. Follow the steps to install the dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILheUROOhprv",
        "colab_type": "text"
      },
      "source": [
        "Now that you installed Spark and Java in Colab, it is time to set the environment path which enables you to run Pyspark in your Colab environment. Set the location of Java and Spark by running the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1b8k_OVf2QF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwrqMk3HiMiE",
        "colab_type": "text"
      },
      "source": [
        "Run a local spark session to test your installation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Uz1NL4gHFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEb4HTRwiaJx",
        "colab_type": "text"
      },
      "source": [
        "Congrats! Your Colab is ready to run Pyspark.\n",
        "\n",
        "# Read input text file to RDD \n",
        "\n",
        "The first step to use Pyspark is reading input text file to resilient distributed dataset (RDD) provides by Spark as the primary abstraction.\n",
        "\n",
        "Download the input data from [here](https://raw.githubusercontent.com/umddm/ECE795_Homeworks/master/clust_B.txt) and keep it in the Colab document by the following command. The input data can also be downloaded by the link."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAISFqHXf7dt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/umddm/ECE795_Homeworks/master/clust_B.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNsM_jHqrjBg",
        "colab_type": "text"
      },
      "source": [
        "Check the input data correctly in the system by the following command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m606eNuQgA82",
        "colab_type": "code",
        "outputId": "79e1c8b6-e781-426e-94fe-e56a6d23a782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clust_B.txt  spark-2.4.4-bin-hadoop2.7\n",
            "sample_data  spark-2.4.4-bin-hadoop2.7.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21D9EANUvnwF",
        "colab_type": "text"
      },
      "source": [
        "Now that we have input data, we can start to do the homework. \n",
        "\n",
        "## Question 1: Use sc.textFile to read the provided input data and split different fields.\n",
        "\n",
        "### Expected output of rdd.take(10):\n",
        "```\n",
        "['23785', '20305', 'i love you', '43']\n",
        "['7', '7', 'hate that i love you so', '36543']\n",
        "['1100', '453', 'hate that i love you', '81168']\n",
        "['6', '6', 'that i love you', '950238']\n",
        "['21041', '18574', 'yes we can', '1485']\n",
        "['308', '302', 'yes we can yes we can', '43112']\n",
        "['262', '249', 'yes we will', '175567']\n",
        "['6', '6', 'we can we will', '555912']\n",
        "['8', '8', 'yes we can and finally yes we will', '639024']\n",
        "['15', '15', 'yes we can and yes we will', '639122']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZeJ7WQCgM8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sc.textFile\n",
        "from pyspark import SparkConf, SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "#Question_1:\n",
        "\n",
        "#Fill out here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3vYyp5dwOm_",
        "colab_type": "text"
      },
      "source": [
        "## Question 2: Convert all the elements in the first and second columns of the RDD to integer type and save in \"Column1.txt\" and \"Column2.txt\".\n",
        "\n",
        "Please do not upload the generated output files to Blackboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eja1BLiaTThT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Question_2\n",
        "\n",
        "#Fill out here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FHVHVQjd4P1",
        "colab_type": "text"
      },
      "source": [
        "## Question 3: Please output the number of elements in the RDD with 'love', 'will' and 'university' separately.\n",
        "For example, a valid element can be 'love', 'I will', 'university of miami' or 'william' (will is a substring of the words)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJsxix2Xd3-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Question_3\n",
        "\n",
        "#Fill out here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIy1IL965Ogm",
        "colab_type": "text"
      },
      "source": [
        "## Question 4: Please utilize the following action RDD operation `sum` to compute how many times the word 'will' appears in the given input file.\n",
        "\n",
        "sum() is an action type of RDD operation, which generate the sum of all the elements in RDD.\n",
        "For example: If rdd includes elements of 1, 2, and 3, rdd.sum() will return 6 as the output.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OmbLiCQ5ctu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Question_4\n",
        "\n",
        "#Fill out here"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
